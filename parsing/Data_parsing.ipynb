{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4zq6V3zN4tIo"
   },
   "outputs": [],
   "source": [
    "# Устанавливаем необходимые библиотеки\n",
    "!pip install requests\n",
    "!pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "Uude2E6D5Cys"
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import xml.etree.ElementTree as ET\n",
    "import requests\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "ti_5hqMzhSgN"
   },
   "outputs": [],
   "source": [
    "# Заголовки, которые пригодятся в следующей функции\n",
    "\n",
    "cookies_1 = {\n",
    "    'session-cookie': '17a3d917fe3f6e7ea9ec545f80267f93d5b416b6059a9d333fcfde712ea74e19b74449acb6e0ca468b7edc9dbf588fa8',\n",
    "    'PHPSESSID': 'f70ef57a430ad16c612f6d0530e8b0dc',\n",
    "    'sputnik_session': '1703443780994|10',\n",
    "}\n",
    "\n",
    "headers_1 = {\n",
    "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',\n",
    "    'Accept-Language': 'ru,en;q=0.9',\n",
    "    'Cache-Control': 'max-age=0',\n",
    "    'Connection': 'keep-alive',\n",
    "    # 'Cookie': 'session-cookie=17a3d917fe3f6e7ea9ec545f80267f93d5b416b6059a9d333fcfde712ea74e19b74449acb6e0ca468b7edc9dbf588fa8; PHPSESSID=f70ef57a430ad16c612f6d0530e8b0dc; sputnik_session=1703443780994|4',\n",
    "    'Upgrade-Insecure-Requests': '1',\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.5993.771 YaBrowser/23.11.2.771 Yowser/2.5 Safari/537.36',\n",
    "}\n",
    "\n",
    "cookies_2 = {\n",
    "    'izbSession': '64b5b4e6-2f81-4b96-8c7e-e074bf77ddc0',\n",
    "    'session-cookie': '179ef5084becc674cb4eb0b280267f9384308ec68e34b70f3eec65bab961a0ae57996adf4a546562c8843651c3afb13c',\n",
    "    'izbFP': 'a289f9847ccd5273811bce9953e2894f',\n",
    "    'JSESSIONID': 'b317e78f92f2a9363fa1963e3817',\n",
    "}\n",
    "\n",
    "headers_2 = {\n",
    "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',\n",
    "    'Accept-Language': 'ru,en;q=0.9',\n",
    "    'Cache-Control': 'max-age=0',\n",
    "    'Connection': 'keep-alive',\n",
    "    'Content-Type': 'application/x-www-form-urlencoded',\n",
    "    # 'Cookie': 'izbSession=64b5b4e6-2f81-4b96-8c7e-e074bf77ddc0; session-cookie=179ef5084becc674cb4eb0b280267f9384308ec68e34b70f3eec65bab961a0ae57996adf4a546562c8843651c3afb13c; izbFP=a289f9847ccd5273811bce9953e2894f; JSESSIONID=b317e78f92f2a9363fa1963e3817',\n",
    "    'Origin': 'http://www.vybory.izbirkom.ru',\n",
    "    'Referer': 'http://www.vybory.izbirkom.ru/region/izbirkom',\n",
    "    'Upgrade-Insecure-Requests': '1',\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.5993.2471 YaBrowser/23.11.0.2471 Yowser/2.5 Safari/537.36',\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция для получения кода сайта, в качестве аргументов:\n",
    "1. url - ссылка на страницу\n",
    "2. headers, cookies - заголовки, чтобы сервер распознавал нас как пользователя"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bGSk4nZd5Cqa"
   },
   "outputs": [],
   "source": [
    "def fetch_data(url, headers, cookies):\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, cookies=cookies, verify=False, timeout=5)\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        print(\"Connection refused\")\n",
    "    else:\n",
    "        print(response.status_code)\n",
    "        return (response, response.status_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Следующие три функции написаны для парсинга данных с разных сайтов и записи их в csv файлы. Каждая принимает на вход по два аргумента: \n",
    "1. Обработанный вид (код) сайта\n",
    "2. Имя csv файла, в который будет записан соответствующий датасет. \n",
    "\n",
    "Первая функция предназначена для сайта cikrf.ru. Все датасеты с этого сайта записаны в формате xml."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "5Raps7u3648S"
   },
   "outputs": [],
   "source": [
    "def parse_and_write_from_xml_to_csv(xml_content, csv_filename):\n",
    "    root = ET.fromstring(xml_content)\n",
    "\n",
    "    # Открываем файл CSV для записи\n",
    "    with open(csv_filename, 'w', encoding=\"utf-8\", newline='') as csvfile:\n",
    "\n",
    "        csv_writer = csv.writer(csvfile, delimiter=';')\n",
    "\n",
    "        # Записываем заголовки CSV\n",
    "        headers_of_csv_file = [\"name\"] + [child[0].text for child in root[0]]\n",
    "        csv_writer.writerow(headers_of_csv_file)\n",
    "\n",
    "        # Записываем данные из XML в CSV\n",
    "        for parent in root:\n",
    "            data_row = [parent.attrib[\"name\"]] + [child[1].text if child[1] is not None else '' for child in parent]\n",
    "            csv_writer.writerow(data_row)\n",
    "\n",
    "        print(f\"Данные успешно записаны в {csv_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JVEFAMbiENgW"
   },
   "source": [
    "Следующий парсер написан для сайта vybory.izbirkom.ru, так как сайт cikrf.ru не содержит данные о выборах раньше 2011 года. В таблицах избиркома заголовками являются названия регионов, а первая колонка содержит характеристики типа: \"Число избирателей, внесенных в списки избирателей\" и т.п. Однако это неудобное представление, поэтому пришлось условно транспонировать таблицу, чтобы характеристики были на месте заголовков."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "71JSo_2v90LF"
   },
   "outputs": [],
   "source": [
    "def parse_and_write_from_html_to_csv(html_content, csv_filename):\n",
    "    soup = BeautifulSoup(html_content, 'lxml')\n",
    "\n",
    "    # Достаём заголовки будущего файла\n",
    "    list_of_head = soup.find('table', id='fix-columns-table').find_all('td', class_='second-fix-col')\n",
    "    native_headers = [list_of_head[i].text for i in range(0, len(list_of_head))]\n",
    "    headers_of_csv_file = [\"name\"] + native_headers\n",
    "\n",
    "    # Достаём \"name\" из изначальной таблицы\n",
    "    soup_f_column = soup.find('table', id='fix-columns-table').find('thead').find_all('a')\n",
    "    first_column = [soup.find('table', id='fix-columns-table').find('thead').find('b').text] + [soup_f_column[i].text for i in range(0, len(soup_f_column))]\n",
    "\n",
    "    # Создаём и заполняем таблицу строчками из изначальной таблицы\n",
    "    data = []\n",
    "    data.append(first_column)\n",
    "\n",
    "    new_soup = soup.find('table', id='fix-columns-table').find('tbody').find_all('tr')\n",
    "\n",
    "    for tr in new_soup:\n",
    "        rows = tr.find_all('b')\n",
    "        if len(rows) != 0:\n",
    "            pr_rows = [rows[i].text if rows[i] is not None else '' for i in range(0, len(rows))]\n",
    "            data.append(pr_rows)\n",
    "\n",
    "    # Открываем файл CSV для записи\n",
    "    with open(csv_filename, 'w', encoding=\"utf-8\", newline='') as csvfile:\n",
    "\n",
    "        csv_writer = csv.writer(csvfile, delimiter=';')\n",
    "\n",
    "        # Записываем заголовки CSV\n",
    "        csv_writer.writerow(headers_of_csv_file)\n",
    "\n",
    "        # Записываем данные из html в CSV\n",
    "        for i in range(0, len(data[0])):\n",
    "            data_row = []\n",
    "            for j in range(0, len(data)):\n",
    "                data_row.append(data[j][i])\n",
    "\n",
    "            csv_writer.writerow(data_row)\n",
    "\n",
    "        print(f\"Данные успешно записаны в {csv_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AjhvZV8JKkvZ"
   },
   "source": [
    "Этот парсер написан специально для сводных результатов выборов президента РФ в 2004 году, так как код таблицы отличается по стилю от кодов остальных таблиц."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "FbpN2bjxKQJg"
   },
   "outputs": [],
   "source": [
    "def parse_2004_elections_and_write_from_html_to_csv(html_content, csv_filename):\n",
    "    soup = BeautifulSoup(html_content, 'lxml')\n",
    "    soup_table = soup.find_all('table', class_='table-bordered')[1]\n",
    "\n",
    "    # Достаём заголовки будущего файла\n",
    "    h_soup = soup_table.find_all('td', class_='text-left')\n",
    "    native_headers = [h_soup[i].find('nobr').text for i in range(0, len(h_soup))]\n",
    "    headers_of_csv_file = [\"name\"] + native_headers\n",
    "\n",
    "    # Достаём строчку с суммарными данными (по всей стране и не только)\n",
    "    sum_row_soup = soup_table.find('td', style='vertical-align: baseline;').find_all('td', class_='text-right')\n",
    "    sum_row = [\"Сумма\"] + [sum_row_soup[i].find('b').text for i in range(0, len(sum_row_soup))]\n",
    "\n",
    "    # Достаём \"name\" из изначальной таблицы\n",
    "    soup_f_column = soup_table.find_all('table', class_='table-bordered')[1].find_all('a')\n",
    "    first_column = [soup_f_column[i].text for i in range(0, len(soup_f_column))]\n",
    "\n",
    "    # Создаём и заполняем таблицу строчками из изначальной таблицы\n",
    "    data = []\n",
    "    data.append(first_column)\n",
    "    table = soup_table.find_all('table', class_='table-striped')[1].find_all('tr')[1:]\n",
    "\n",
    "    for tr in table:\n",
    "        rows = tr.find_all('b')\n",
    "        if len(rows) != 0:\n",
    "            pr_rows = [rows[i].text if rows[i] is not None else '' for i in range(0, len(rows))]\n",
    "            data.append(pr_rows)\n",
    "\n",
    "    # Открываем файл CSV для записи\n",
    "    with open(csv_filename, 'w', encoding=\"utf-8\", newline='') as csvfile:\n",
    "\n",
    "        csv_writer = csv.writer(csvfile, delimiter=';')\n",
    "\n",
    "        # Записываем заголовки CSV\n",
    "        csv_writer.writerow(headers_of_csv_file)\n",
    "\n",
    "        # Записываем строчку с суммарными данными\n",
    "        csv_writer.writerow(sum_row)\n",
    "\n",
    "        # Записываем данные из html в CSV\n",
    "        for i in range(0, len(data[0])):\n",
    "            data_row = []\n",
    "            for j in range(0, len(data)):\n",
    "                data_row.append(data[j][i])\n",
    "\n",
    "            csv_writer.writerow(data_row)\n",
    "\n",
    "        print(f\"Данные успешно записаны в {csv_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "Ug6Lk6e_SvML"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "Данные успешно записаны в Результаты_выборов_президента_2012.csv\n",
      "200\n",
      "Данные успешно записаны в Результаты_выборов_в_думу_2011.csv\n",
      "200\n",
      "Данные успешно записаны в Результаты_выборов_в_думу_2016.csv\n",
      "200\n",
      "Данные успешно записаны в Результаты_выборов_в_думу_2016_по_фед.csv\n",
      "200\n",
      "Данные успешно записаны в Результаты_выборов_президента_2018.csv\n",
      "200\n",
      "Данные успешно записаны в Результаты_выборов_в_думу_2021_по_фед.csv\n",
      "200\n",
      "Данные успешно записаны в Результаты_выборов_в_думу_2021.csv\n",
      "200\n",
      "Данные успешно записаны в Результаты_выборов_в_думу_2003_по_фед.csv\n",
      "200\n",
      "Данные успешно записаны в Результаты_выборов_в_думу_2007_по_фед.csv\n",
      "200\n",
      "Данные успешно записаны в Результаты_выборов_президента_2008.csv\n",
      "200\n",
      "Данные успешно записаны в Результаты_выборов_президента_2004.csv\n"
     ]
    }
   ],
   "source": [
    "# Ну и наконец создаем списки, которые определяют, в какой файл записываем данные по url-ссылке\n",
    "list_of_ind_filenames = [(7, \"Результаты_выборов_президента_2012\"),\n",
    "                         (15, \"Результаты_выборов_в_думу_2011\"),\n",
    "                         (21, \"Результаты_выборов_в_думу_2016\"),\n",
    "                         (23, \"Результаты_выборов_в_думу_2016_по_фед\"),\n",
    "                         (41, \"Результаты_выборов_президента_2018\"),\n",
    "                         (84, \"Результаты_выборов_в_думу_2021_по_фед\"),\n",
    "                         (82, \"Результаты_выборов_в_думу_2021\")]\n",
    "list_of_url_filenames = [(\"http://www.vybory.izbirkom.ru/region/region/izbirkom?action=show&root=1&tvd=100100095621&vrn=100100095619&region=0&global=1&sub_region=0&prver=0&pronetvd=0&vibid=100100095621&type=233\",\"Результаты_выборов_в_думу_2003_по_фед\"),\n",
    "                         (\"http://www.vybory.izbirkom.ru/region/region/izbirkom?action=show&root=1&tvd=100100021960186&vrn=100100021960181&region=0&global=1&sub_region=0&prver=0&pronetvd=null&vibid=100100021960186&type=233\", \"Результаты_выборов_в_думу_2007_по_фед\"),\n",
    "                         (\"http://www.vybory.izbirkom.ru/region/region/izbirkom?action=show&root=1&tvd=100100022249920&vrn=100100022176412&region=0&global=1&sub_region=0&prver=0&pronetvd=null&vibid=100100022249920&type=227\", \"Результаты_выборов_президента_2008\")]\n",
    "elections_2004 = (\"http://www.vybory.izbirkom.ru/region/region/izbirkom?action=show&root=1&tvd=1001000882951&vrn=1001000882950&region=0&global=1&sub_region=0&prver=0&pronetvd=null&vibid=1001000882951&type=227\", \"Результаты_выборов_президента_2004\")\n",
    "\n",
    "for pair in list_of_ind_filenames:\n",
    "    csv_filename = f\"{pair[1]}.csv\"\n",
    "    func_ret = fetch_data(f\"http://www.cikrf.ru/opendata/{pair[0]}.xml\", headers_1, cookies_1)\n",
    "    if func_ret[1] == 200:\n",
    "        parse_and_write_from_xml_to_csv(func_ret[0].content, csv_filename)\n",
    "\n",
    "for pair in list_of_url_filenames:\n",
    "    csv_filename = f\"{pair[1]}.csv\"\n",
    "    func_ret = fetch_data(pair[0], headers_2, cookies_2)\n",
    "    if func_ret[1] == 200:\n",
    "        parse_and_write_from_html_to_csv(func_ret[0].text, csv_filename)\n",
    "\n",
    "\n",
    "csv_filename = f\"{elections_2004[1]}.csv\"\n",
    "func_ret = fetch_data(elections_2004[0], headers_2, cookies_2)\n",
    "if func_ret[1] == 200:\n",
    "    parse_2004_elections_and_write_from_html_to_csv(func_ret[0].text, csv_filename)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
